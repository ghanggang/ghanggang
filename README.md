<p>
 üëã&nbsp; Hi there! I'm Jaeghang Choi. I'm enrolled in the <b/>Master of Computer Science</b> in Kookmin University.
</p>

### Contact 
<p>
  <a href="https://manchann.tistory.com/" target="_blank"><img src="https://img.shields.io/badge/Blog-003DAD?style=flat-square&logo=Blogger&logoColor=white"/></a>
  <a href="mailto:workd.official@gmail.com" target="_blank"><img src="https://img.shields.io/badge/workd.official@gmail.com-EA4335?style=flat-square&logo=Gmail&logoColor=white"/></a>
</p>

## My Research History
### Deep Learning Inference (2021.08.19 ~ )
I'm working on the impact of <b/>different hardware</b> (CPU, GPU, TPU, AWS Inferentia ...) on deep learning inference.<br/>
[What is Deep Learning Inference?](https://manchann.tistory.com/16) <br/>

### Serverless Computing (2020.03.18 ~ 2021.08.18)
I researched Serverless Computing when I was undergraduate. <br/><br/>
üçÄI published a <b/>Paper</b> with my professor about..<br/>
[Evaluation of Network File System as a Shared Data Storage in Serverless Computing](https://dl.acm.org/doi/10.1145/3429880.3430096)

## ML Serving System ÏµúÏã† ÎèôÌñ• ÌååÏïÖÏùÑ ÏúÑÌïú ÎÖºÎ¨∏ ÏÑ∏ÎØ∏ÎÇò
- [Enabling Cost-Effective, SLO-Aware Machine Learning Inference Serving on Public Cloud](https://docs.google.com/presentation/d/18rqWw-2Z_zLGKfo1OzxPIzqfk0Rg-xEd/edit?usp=sharing&ouid=107371387931093249059&rtpof=true&sd=true)
- [INFaaS: A Model-less and Managed Inference Serving System](https://docs.google.com/presentation/d/1IO8GxKQBvbc23C5JN_Ia1hYAl-W9BLRq/edit?usp=sharing&ouid=107371387931093249059&rtpof=true&sd=true)
- [Characterizing the Deep Neural Networks Inference Performance of Mobile Applications](https://docs.google.com/presentation/d/1Jwb78RVbdbcnTdnW7BosBzgFWI9s7r3Z/edit?usp=sharing&ouid=107371387931093249059&rtpof=true&sd=true)
- [Morphling: Fast, Near-Optimal Auto-Configuration for Cloud-Native Model Serving](https://docs.google.com/presentation/d/16IVvgsw2-8BrG-86zm5Yzq68LB367YG0/edit?usp=sharing&ouid=107371387931093249059&rtpof=true&sd=true)
- [MLProxy: SLA-Aware Reverse Proxy for Machine Learning Inference Serving on Serverless Computing Platforms](https://docs.google.com/presentation/d/1B6QRY2k3UNUC4IeNZD3ySIw5VePDdwDC/edit?usp=sharing&ouid=107371387931093249059&rtpof=true&sd=true)
- [Llama: A Heterogeneous & Serverless Framework for Auto-Tuning Video Analytics Pipelines](https://docs.google.com/presentation/d/1m2xb_BBj69AaRwVtF3KoZhl05d0gjqMz/edit?usp=sharing&ouid=107371387931093249059&rtpof=true&sd=true)
- [An efficient and flexible inference system for serving heterogeneous ensembles of deep neural networks](https://docs.google.com/presentation/d/1dOO2Zn1kurPlkBtyq9QmabH4ZgwifCRcYVLCgKLLPn8/edit?usp=sharing)
- [Serving Machine Learning Inference Using Heterogeneous Hardware](https://docs.google.com/presentation/d/1VqD5IwPhgK7ZXBD5Iwqpvid05YlwBf-JcCdtyC7861M/edit?usp=sharing)
